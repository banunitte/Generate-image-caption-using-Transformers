{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_for_image_cationing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM31y_qwCH-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3c8f7219-603e-4596-a18a-646a1f7f72f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2g4LsXGCTj_"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf0zSd9L4Ifk"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POj0gzm-4LV0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4f82bbf5-0f9d-42ff-d6b3-3dda1f5d9a35"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoTZXJBrTuqo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca12b380-152a-4c2a-9073-417df34a44e4"
      },
      "source": [
        "image_features_extract_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, None, None, 3 864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, None, None, 3 96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, None, None, 3 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, None, None, 3 9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, None, None, 3 96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, None, None, 3 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, None, None, 6 18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, None, None, 6 192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, None, None, 6 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 6 0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, None, None, 8 5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, None, None, 8 240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, None, None, 8 0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, None, None, 1 138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, None, None, 1 576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, None, None, 1 0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 1 0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, None, None, 6 192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, None, None, 6 0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, None, None, 4 9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, None, None, 9 55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, None, None, 4 144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, None, None, 9 288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, None, None, 4 0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, None, None, 9 0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, None, None, 1 0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, None, None, 6 12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, None, None, 6 76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, None, None, 9 82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, None, None, 3 6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, None, None, 6 192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, None, None, 6 192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, None, None, 9 288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, None, None, 3 96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, None, None, 6 0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, None, None, 6 0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, None, None, 9 0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, None, None, 3 0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, None, None, 2 0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, None, None, 6 192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, None, None, 6 0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, None, None, 4 12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, None, None, 9 55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, None, None, 4 144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, None, None, 9 288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, None, None, 4 0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, None, None, 9 0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, None, None, 2 0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, None, None, 6 76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, None, None, 9 82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, None, None, 6 16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, None, None, 6 192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, None, None, 6 192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, None, None, 9 288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, None, None, 6 192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, None, None, 6 0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, None, None, 6 0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, None, None, 9 0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, None, None, 6 0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, None, None, 2 0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, None, None, 6 192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, None, None, 6 0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, None, None, 4 13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, None, None, 9 55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, None, None, 4 144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, None, None, 9 288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, None, None, 4 0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, None, None, 9 0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, None, None, 2 0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, None, None, 6 76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, None, None, 9 82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, None, None, 6 18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, None, None, 6 192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, None, None, 6 192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, None, None, 9 288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, None, None, 6 192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, None, None, 6 0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, None, None, 6 0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, None, None, 9 0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, None, None, 6 0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, None, None, 2 0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, None, None, 6 18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, None, None, 6 192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, None, None, 6 0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, None, None, 9 55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, None, None, 9 288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, None, None, 9 0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, None, None, 3 995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, None, None, 9 82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, None, None, 3 1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, None, None, 9 288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, None, None, 3 0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, None, None, 9 0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, None, None, 7 0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, None, None, 1 384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, None, None, 1 0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, None, None, 1 114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, None, None, 1 384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, None, None, 1 0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, None, None, 1 114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, None, None, 1 384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, None, None, 1 384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, None, None, 1 0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, None, None, 1 0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, None, None, 1 114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, None, None, 1 114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, None, None, 1 384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, None, None, 1 384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, None, None, 1 0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, None, None, 1 0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, None, None, 7 0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, None, None, 1 147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, None, None, 1 172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, None, None, 1 172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, None, None, 1 576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, None, None, 1 576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, None, None, 1 576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, None, None, 1 576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, None, None, 1 0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, None, None, 1 0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, None, None, 1 0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, None, None, 1 0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, None, None, 7 0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, None, None, 1 480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, None, None, 1 0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, None, None, 1 179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, None, None, 1 480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, None, None, 1 0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, None, None, 1 179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, None, None, 1 480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, None, None, 1 480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, None, None, 1 0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, None, None, 1 0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, None, None, 1 179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, None, None, 1 179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, None, None, 1 480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, None, None, 1 480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, None, None, 1 0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, None, None, 1 0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, None, None, 7 0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, None, None, 1 147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, None, None, 1 215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, None, None, 1 215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, None, None, 1 576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, None, None, 1 576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, None, None, 1 576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, None, None, 1 576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, None, None, 1 0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, None, None, 1 0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, None, None, 1 0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, None, None, 1 0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, None, None, 7 0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, None, None, 1 480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, None, None, 1 0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, None, None, 1 179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, None, None, 1 480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, None, None, 1 0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, None, None, 1 179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, None, None, 1 480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, None, None, 1 480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, None, None, 1 0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, None, None, 1 0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, None, None, 1 179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, None, None, 1 179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, None, None, 1 480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, None, None, 1 480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, None, None, 1 0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, None, None, 1 0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, None, None, 7 0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, None, None, 1 147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, None, None, 1 215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, None, None, 1 215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, None, None, 1 576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, None, None, 1 576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, None, None, 1 576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, None, None, 1 576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, None, None, 1 0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, None, None, 1 0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, None, None, 1 0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, None, None, 1 0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, None, None, 7 0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, None, None, 1 576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, None, None, 1 0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, None, None, 1 258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, None, None, 1 576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, None, None, 1 0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, None, None, 1 258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, None, None, 1 576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, None, None, 1 576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, None, None, 1 0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, None, None, 1 0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, None, None, 1 258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, None, None, 1 258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, None, None, 1 576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, None, None, 1 576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, None, None, 1 0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, None, None, 1 0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, None, None, 7 0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, None, None, 1 258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, None, None, 1 258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, None, None, 1 147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, None, None, 1 576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, None, None, 1 576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, None, None, 1 576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, None, None, 1 576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, None, None, 1 0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, None, None, 1 0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, None, None, 1 0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, None, None, 1 0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, None, None, 7 0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, None, None, 1 576         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, None, None, 1 0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, None, None, 1 258048      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, None, None, 1 576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, None, None, 1 0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, None, None, 1 258048      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, None, None, 1 576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, None, None, 1 576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, None, None, 1 0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, None, None, 1 0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, None, None, 3 552960      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, None, None, 1 331776      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, None, None, 3 960         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, None, None, 1 576         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, None, None, 3 0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, None, None, 1 0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, None, None, 1 0           activation_165[0][0]             \n",
            "                                                                 activation_169[0][0]             \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, None, None, 4 573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, None, None, 4 1344        conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, None, None, 4 0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, None, None, 3 491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, None, None, 3 1548288     activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, None, None, 3 1152        conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, None, None, 3 1152        conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, None, None, 3 0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, None, None, 3 0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, None, None, 3 442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, None, None, 3 442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, None, None, 3 442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, None, None, 3 442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, None, None, 1 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, None, None, 3 409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, None, None, 3 1152        conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, None, None, 3 1152        conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, None, None, 3 1152        conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, None, None, 3 1152        conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, None, None, 1 245760      average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, None, None, 3 960         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, None, None, 3 0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, None, None, 3 0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, None, None, 3 0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, None, None, 3 0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, None, None, 1 576         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, None, None, 3 0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_172[0][0]             \n",
            "                                                                 activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 7 0           activation_176[0][0]             \n",
            "                                                                 activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, None, None, 1 0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, None, None, 2 0           activation_170[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, None, None, 4 917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, None, None, 4 1344        conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, None, None, 4 0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, None, None, 3 786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, None, None, 3 1548288     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, None, None, 3 1152        conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, None, None, 3 1152        conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, None, None, 3 0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, None, None, 3 0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, None, None, 3 442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, None, None, 3 442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, None, None, 3 442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, None, None, 3 442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, None, None, 2 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, None, None, 3 655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, None, None, 3 1152        conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, None, None, 3 1152        conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, None, None, 3 1152        conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, None, None, 3 1152        conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, None, None, 1 393216      average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, None, None, 3 960         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, None, None, 3 0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, None, None, 3 0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, None, None, 3 0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, None, None, 3 0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, None, None, 1 576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, None, None, 3 0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_181[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, None, 7 0           activation_185[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, None, None, 1 0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, None, None, 2 0           activation_179[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 activation_187[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AWt3f34Na9"
      },
      "source": [
        "!pip install -q tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFUR3pad4QDO"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hCIvVCA6Yd4"
      },
      "source": [
        "import pickle\n",
        "with open('/content/gdrive/My Drive/full_data/train_captions.pkl','rb') as f:\n",
        "  train_captions = pickle.load(f)\n",
        "with open('/content/gdrive/My Drive/full_data/img_name_vector.pkl','rb') as f:\n",
        "  img_name_vector = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-igSXOqAKVA"
      },
      "source": [
        "with open('/content/gdrive/My Drive/full_data/train_captions.pkl','rb') as f:\n",
        "  train_captions = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz8H5PxL8eIA"
      },
      "source": [
        "annotation_file = '/content/gdrive/My Drive/image_caption/annotations/annotations/captions_train2014.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "PATH = '/content/gdrive/My Drive/image_caption/train2014/train2014/'\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "    try:\n",
        "      if os.path.exists(full_coco_image_path):\n",
        "        all_img_name_vector.append(full_coco_image_path)\n",
        "        all_captions.append(caption)\n",
        "    except:\n",
        "      continue\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIb39kFe7ZDT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "548f6cc5-734f-4bc9-e4a9-bf88b7524b5d"
      },
      "source": [
        "len()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/image_caption/train2014/train2014/COCO_train2014_000000467180.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0UL-W3L4Y2C"
      },
      "source": [
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en for en in train_captions), target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaTipIEXBu4F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f95371a0-651f-4553-c566-98c84a799d31"
      },
      "source": [
        "len(train_captions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51719"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f0_Tvn17hIA"
      },
      "source": [
        "captions = []\n",
        "for i,item in enumerate(train_captions):\n",
        "  captions.append(tokenizer.encode(item))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGyNGDdEB75o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52278318-101b-4141-f446-7f16b112c98b"
      },
      "source": [
        "train_captions[45902]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'<start> A woman leading a horse inside of a building. <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6tjYef1B-hX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c19e5dfa-b555-47ea-81eb-d87d9bb21c8b"
      },
      "source": [
        "tokenizer.decode(captions[45902])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'<start> A woman leading a horse inside of a building. <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSHtg2Ol4VYR"
      },
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 8000\n",
        "\n",
        "# Create training and validation sets using an 80-20 split\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    captions,\n",
        "                                                                    test_size=0.01,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgnZWbodCPvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfe5032b-a6ba-412f-fbd8-449145797d75"
      },
      "source": [
        "len(img_name_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwxfY3-ACpZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b845602-5aca-4e00-d2e9-255fed15a38f"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8607"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN3RY3UnCjPv"
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "MAX_LENGTH = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed1Il9xYzOL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "736bf50a-a342-4b8f-809c-2d57ae9ebcdb"
      },
      "source": [
        "num_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjyoF0KBC3MC"
      },
      "source": [
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpXUqOCTDNvo"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) >= 1,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-_fMp-E38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ed9c0ab-44c5-48c6-9f54-7bb5971104bb"
      },
      "source": [
        "type(cap_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9gvNgDFF0VT"
      },
      "source": [
        "cap_train = tf.keras.preprocessing.sequence.pad_sequences(cap_train, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBesr8XOE2Mh"
      },
      "source": [
        "train_examples = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6jBiQ6_GQuJ"
      },
      "source": [
        "#train_dataset = train_examples.filter(filter_max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV1sKWM7Da-h"
      },
      "source": [
        "dataset = train_examples.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIRU-enqH0hw"
      },
      "source": [
        "train_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxFHEhiVIQuq"
      },
      "source": [
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Aue8KALR-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5caa92af-fa14-401c-c59b-81fcd2613b52"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u27UWSPDLAsU"
      },
      "source": [
        "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ64sx1wLJJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d65c6d7-a29e-4d4f-a723-23725393717a"
      },
      "source": [
        "tar.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 68])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viYp10DOMboU"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nrNykpLMkg8"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4tOx444aiXk"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzsawbHcMvLb"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SstE8EOHM0U1"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n0cCdgWM3-4"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqTGOZTKM9TA"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdplasamNBJJ"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXOLDs6eNFei"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62guVhEJNLSH"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMZ5wInLNw2u"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "               target_vocab_size, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = EncoderLayer(d_model)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvzcTVT6V5aU"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "#input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer.vocab_size \n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKhAR_gFWGiC"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PqPvkJXWJOu"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLBPNtwWLmw"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keowOBTfhfLb"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtQbWdaDWNXd"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6sbOYuWPg3"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size,  \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEdm45maXo7v"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7FUKlgXsDP"
      },
      "source": [
        "checkpoint_path = \"/content/gdrive/My Drive/transformer_caption/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8SCSgxnYJIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79f50156-ca6e-4477-aa90-c39c71ea18f1"
      },
      "source": [
        "ckpt_manager"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpoint_management.CheckpointManager at 0x7ff944f07c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRXkyYo9YNuh"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tf.ones([64, 64], tf.int32), tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKg_q1GJjmwr"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tf.ones([tar_inp.shape[0], 64], tf.int32), tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LttL9BMyG-c9"
      },
      "source": [
        "## Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_25aFDF_eZ27"
      },
      "source": [
        "tar_inp = tar[:, :-1]\n",
        "tar_real = tar[:, 1:]\n",
        "  \n",
        "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(tf.ones([1, 64], tf.int32), tar_inp)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpBt0KFUzsb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5846ccae-42fe-47e8-aa28-696cb4232db2"
      },
      "source": [
        "tar_inp.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lalNx9znfZ0k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4180eccc-dba4-4f04-db2b-4a2399c86866"
      },
      "source": [
        "enc_padding_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 1, 1, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsaAAITqenXF"
      },
      "source": [
        "predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkZuPM-ngKD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57d6b94b-3061-43d9-f721-3c0d881980fd"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 67, 8607])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2POGCB1zEOJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96ca78b3-00ac-4f1f-dae1-e15c82ed9b70"
      },
      "source": [
        "tar.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 68])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LlfKQQzy_Ri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "043ec816-930a-4514-d4ef-065cc8dce5cb"
      },
      "source": [
        "inp.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 64, 2048])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dmdcmPKHB2E"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wddUvt07Ysqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6286fe49-4380-48df-ff70-ab9e1d3a9ebc"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 9.0868 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 9.0075 Accuracy 0.0008\n",
            "Epoch 1 Batch 100 Loss 8.8683 Accuracy 0.0087\n",
            "Epoch 1 Batch 150 Loss 8.7114 Accuracy 0.0169\n",
            "Epoch 1 Batch 200 Loss 8.5190 Accuracy 0.0268\n",
            "Epoch 1 Batch 250 Loss 8.2882 Accuracy 0.0372\n",
            "Epoch 1 Batch 300 Loss 8.0317 Accuracy 0.0447\n",
            "Epoch 1 Batch 350 Loss 7.7645 Accuracy 0.0501\n",
            "Epoch 1 Batch 400 Loss 7.4944 Accuracy 0.0549\n",
            "Epoch 1 Batch 450 Loss 7.2334 Accuracy 0.0590\n",
            "Epoch 1 Batch 500 Loss 6.9856 Accuracy 0.0626\n",
            "Epoch 1 Batch 550 Loss 6.7473 Accuracy 0.0659\n",
            "Epoch 1 Batch 600 Loss 6.5270 Accuracy 0.0688\n",
            "Epoch 1 Batch 650 Loss 6.3245 Accuracy 0.0714\n",
            "Epoch 1 Batch 700 Loss 6.1391 Accuracy 0.0738\n",
            "Epoch 1 Batch 750 Loss 5.9737 Accuracy 0.0758\n",
            "Epoch 1 Batch 800 Loss 5.8263 Accuracy 0.0777\n",
            "Epoch 1 Loss 5.8263 Accuracy 0.0777\n",
            "Time taken for 1 epoch: 5880.883199930191 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.4585 Accuracy 0.1056\n",
            "Epoch 2 Batch 50 Loss 3.5076 Accuracy 0.1074\n",
            "Epoch 2 Batch 100 Loss 3.4755 Accuracy 0.1080\n",
            "Epoch 2 Batch 150 Loss 3.4423 Accuracy 0.1084\n",
            "Epoch 2 Batch 200 Loss 3.4168 Accuracy 0.1091\n",
            "Epoch 2 Batch 250 Loss 3.3847 Accuracy 0.1099\n",
            "Epoch 2 Batch 300 Loss 3.3584 Accuracy 0.1103\n",
            "Epoch 2 Batch 350 Loss 3.3271 Accuracy 0.1110\n",
            "Epoch 2 Batch 400 Loss 3.2976 Accuracy 0.1117\n",
            "Epoch 2 Batch 450 Loss 3.2678 Accuracy 0.1123\n",
            "Epoch 2 Batch 500 Loss 3.2396 Accuracy 0.1129\n",
            "Epoch 2 Batch 550 Loss 3.2114 Accuracy 0.1135\n",
            "Epoch 2 Batch 600 Loss 3.1889 Accuracy 0.1140\n",
            "Epoch 2 Batch 650 Loss 3.1638 Accuracy 0.1146\n",
            "Epoch 2 Batch 700 Loss 3.1404 Accuracy 0.1151\n",
            "Epoch 2 Batch 750 Loss 3.1172 Accuracy 0.1155\n",
            "Epoch 2 Batch 800 Loss 3.0940 Accuracy 0.1160\n",
            "Epoch 2 Loss 3.0940 Accuracy 0.1160\n",
            "Time taken for 1 epoch: 241.17562747001648 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.8450 Accuracy 0.1206\n",
            "Epoch 3 Batch 50 Loss 2.7370 Accuracy 0.1238\n",
            "Epoch 3 Batch 100 Loss 2.7299 Accuracy 0.1238\n",
            "Epoch 3 Batch 150 Loss 2.7146 Accuracy 0.1240\n",
            "Epoch 3 Batch 200 Loss 2.7067 Accuracy 0.1243\n",
            "Epoch 3 Batch 250 Loss 2.6983 Accuracy 0.1245\n",
            "Epoch 3 Batch 300 Loss 2.6858 Accuracy 0.1248\n",
            "Epoch 3 Batch 350 Loss 2.6756 Accuracy 0.1249\n",
            "Epoch 3 Batch 400 Loss 2.6641 Accuracy 0.1251\n",
            "Epoch 3 Batch 450 Loss 2.6535 Accuracy 0.1253\n",
            "Epoch 3 Batch 500 Loss 2.6444 Accuracy 0.1255\n",
            "Epoch 3 Batch 550 Loss 2.6349 Accuracy 0.1257\n",
            "Epoch 3 Batch 600 Loss 2.6267 Accuracy 0.1259\n",
            "Epoch 3 Batch 650 Loss 2.6184 Accuracy 0.1261\n",
            "Epoch 3 Batch 700 Loss 2.6087 Accuracy 0.1262\n",
            "Epoch 3 Batch 750 Loss 2.6010 Accuracy 0.1263\n",
            "Epoch 3 Batch 800 Loss 2.5908 Accuracy 0.1265\n",
            "Epoch 3 Loss 2.5908 Accuracy 0.1265\n",
            "Time taken for 1 epoch: 241.46686148643494 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.5177 Accuracy 0.1278\n",
            "Epoch 4 Batch 50 Loss 2.4755 Accuracy 0.1290\n",
            "Epoch 4 Batch 100 Loss 2.4595 Accuracy 0.1293\n",
            "Epoch 4 Batch 150 Loss 2.4535 Accuracy 0.1294\n",
            "Epoch 4 Batch 200 Loss 2.4490 Accuracy 0.1294\n",
            "Epoch 4 Batch 250 Loss 2.4412 Accuracy 0.1296\n",
            "Epoch 4 Batch 300 Loss 2.4359 Accuracy 0.1297\n",
            "Epoch 4 Batch 350 Loss 2.4320 Accuracy 0.1298\n",
            "Epoch 4 Batch 400 Loss 2.4252 Accuracy 0.1299\n",
            "Epoch 4 Batch 450 Loss 2.4211 Accuracy 0.1300\n",
            "Epoch 4 Batch 500 Loss 2.4164 Accuracy 0.1302\n",
            "Epoch 4 Batch 550 Loss 2.4104 Accuracy 0.1302\n",
            "Epoch 4 Batch 600 Loss 2.4080 Accuracy 0.1303\n",
            "Epoch 4 Batch 650 Loss 2.4047 Accuracy 0.1304\n",
            "Epoch 4 Batch 700 Loss 2.3982 Accuracy 0.1305\n",
            "Epoch 4 Batch 750 Loss 2.3936 Accuracy 0.1305\n",
            "Epoch 4 Batch 800 Loss 2.3882 Accuracy 0.1306\n",
            "Epoch 4 Loss 2.3882 Accuracy 0.1306\n",
            "Time taken for 1 epoch: 247.38219618797302 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.3256 Accuracy 0.1227\n",
            "Epoch 5 Batch 50 Loss 2.3313 Accuracy 0.1320\n",
            "Epoch 5 Batch 100 Loss 2.3236 Accuracy 0.1317\n",
            "Epoch 5 Batch 150 Loss 2.3244 Accuracy 0.1319\n",
            "Epoch 5 Batch 200 Loss 2.3211 Accuracy 0.1319\n",
            "Epoch 5 Batch 250 Loss 2.3164 Accuracy 0.1322\n",
            "Epoch 5 Batch 300 Loss 2.3129 Accuracy 0.1322\n",
            "Epoch 5 Batch 350 Loss 2.3113 Accuracy 0.1322\n",
            "Epoch 5 Batch 400 Loss 2.3043 Accuracy 0.1323\n",
            "Epoch 5 Batch 450 Loss 2.3022 Accuracy 0.1324\n",
            "Epoch 5 Batch 500 Loss 2.2969 Accuracy 0.1325\n",
            "Epoch 5 Batch 550 Loss 2.2931 Accuracy 0.1325\n",
            "Epoch 5 Batch 600 Loss 2.2921 Accuracy 0.1326\n",
            "Epoch 5 Batch 650 Loss 2.2894 Accuracy 0.1327\n",
            "Epoch 5 Batch 700 Loss 2.2850 Accuracy 0.1327\n",
            "Epoch 5 Batch 750 Loss 2.2806 Accuracy 0.1328\n",
            "Epoch 5 Batch 800 Loss 2.2771 Accuracy 0.1329\n",
            "Saving checkpoint for epoch 5 at /content/gdrive/My Drive/transformer_caption/train/ckpt-1\n",
            "Epoch 5 Loss 2.2771 Accuracy 0.1329\n",
            "Time taken for 1 epoch: 246.4516568183899 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.2651 Accuracy 0.1348\n",
            "Epoch 6 Batch 50 Loss 2.2499 Accuracy 0.1341\n",
            "Epoch 6 Batch 100 Loss 2.2400 Accuracy 0.1341\n",
            "Epoch 6 Batch 150 Loss 2.2315 Accuracy 0.1340\n",
            "Epoch 6 Batch 200 Loss 2.2307 Accuracy 0.1342\n",
            "Epoch 6 Batch 250 Loss 2.2261 Accuracy 0.1343\n",
            "Epoch 6 Batch 300 Loss 2.2224 Accuracy 0.1343\n",
            "Epoch 6 Batch 350 Loss 2.2160 Accuracy 0.1345\n",
            "Epoch 6 Batch 400 Loss 2.2122 Accuracy 0.1346\n",
            "Epoch 6 Batch 450 Loss 2.2076 Accuracy 0.1346\n",
            "Epoch 6 Batch 500 Loss 2.2029 Accuracy 0.1347\n",
            "Epoch 6 Batch 550 Loss 2.1977 Accuracy 0.1348\n",
            "Epoch 6 Batch 600 Loss 2.1953 Accuracy 0.1349\n",
            "Epoch 6 Batch 650 Loss 2.1919 Accuracy 0.1350\n",
            "Epoch 6 Batch 700 Loss 2.1854 Accuracy 0.1351\n",
            "Epoch 6 Batch 750 Loss 2.1802 Accuracy 0.1352\n",
            "Epoch 6 Batch 800 Loss 2.1762 Accuracy 0.1353\n",
            "Epoch 6 Loss 2.1762 Accuracy 0.1353\n",
            "Time taken for 1 epoch: 242.75463199615479 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.0700 Accuracy 0.1376\n",
            "Epoch 7 Batch 50 Loss 2.1285 Accuracy 0.1367\n",
            "Epoch 7 Batch 100 Loss 2.1135 Accuracy 0.1369\n",
            "Epoch 7 Batch 150 Loss 2.1125 Accuracy 0.1368\n",
            "Epoch 7 Batch 200 Loss 2.1082 Accuracy 0.1371\n",
            "Epoch 7 Batch 250 Loss 2.1033 Accuracy 0.1372\n",
            "Epoch 7 Batch 300 Loss 2.1001 Accuracy 0.1374\n",
            "Epoch 7 Batch 350 Loss 2.0946 Accuracy 0.1374\n",
            "Epoch 7 Batch 400 Loss 2.0909 Accuracy 0.1375\n",
            "Epoch 7 Batch 450 Loss 2.0865 Accuracy 0.1375\n",
            "Epoch 7 Batch 500 Loss 2.0842 Accuracy 0.1376\n",
            "Epoch 7 Batch 550 Loss 2.0785 Accuracy 0.1377\n",
            "Epoch 7 Batch 600 Loss 2.0761 Accuracy 0.1379\n",
            "Epoch 7 Batch 650 Loss 2.0732 Accuracy 0.1379\n",
            "Epoch 7 Batch 700 Loss 2.0669 Accuracy 0.1380\n",
            "Epoch 7 Batch 750 Loss 2.0630 Accuracy 0.1381\n",
            "Epoch 7 Batch 800 Loss 2.0582 Accuracy 0.1382\n",
            "Epoch 7 Loss 2.0582 Accuracy 0.1382\n",
            "Time taken for 1 epoch: 239.77426147460938 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.1429 Accuracy 0.1362\n",
            "Epoch 8 Batch 50 Loss 2.0340 Accuracy 0.1400\n",
            "Epoch 8 Batch 100 Loss 2.0114 Accuracy 0.1398\n",
            "Epoch 8 Batch 150 Loss 2.0032 Accuracy 0.1397\n",
            "Epoch 8 Batch 200 Loss 1.9995 Accuracy 0.1398\n",
            "Epoch 8 Batch 250 Loss 1.9989 Accuracy 0.1400\n",
            "Epoch 8 Batch 300 Loss 1.9925 Accuracy 0.1401\n",
            "Epoch 8 Batch 350 Loss 1.9903 Accuracy 0.1400\n",
            "Epoch 8 Batch 400 Loss 1.9866 Accuracy 0.1402\n",
            "Epoch 8 Batch 450 Loss 1.9830 Accuracy 0.1402\n",
            "Epoch 8 Batch 500 Loss 1.9800 Accuracy 0.1403\n",
            "Epoch 8 Batch 550 Loss 1.9752 Accuracy 0.1404\n",
            "Epoch 8 Batch 600 Loss 1.9730 Accuracy 0.1405\n",
            "Epoch 8 Batch 650 Loss 1.9713 Accuracy 0.1405\n",
            "Epoch 8 Batch 700 Loss 1.9656 Accuracy 0.1407\n",
            "Epoch 8 Batch 750 Loss 1.9621 Accuracy 0.1408\n",
            "Epoch 8 Batch 800 Loss 1.9572 Accuracy 0.1408\n",
            "Epoch 8 Loss 1.9572 Accuracy 0.1408\n",
            "Time taken for 1 epoch: 238.28789067268372 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.7949 Accuracy 0.1392\n",
            "Epoch 9 Batch 50 Loss 1.9251 Accuracy 0.1427\n",
            "Epoch 9 Batch 100 Loss 1.9207 Accuracy 0.1420\n",
            "Epoch 9 Batch 150 Loss 1.9135 Accuracy 0.1421\n",
            "Epoch 9 Batch 200 Loss 1.9105 Accuracy 0.1422\n",
            "Epoch 9 Batch 250 Loss 1.9102 Accuracy 0.1423\n",
            "Epoch 9 Batch 300 Loss 1.9031 Accuracy 0.1425\n",
            "Epoch 9 Batch 350 Loss 1.9010 Accuracy 0.1424\n",
            "Epoch 9 Batch 400 Loss 1.8974 Accuracy 0.1426\n",
            "Epoch 9 Batch 450 Loss 1.8946 Accuracy 0.1426\n",
            "Epoch 9 Batch 500 Loss 1.8914 Accuracy 0.1427\n",
            "Epoch 9 Batch 550 Loss 1.8875 Accuracy 0.1428\n",
            "Epoch 9 Batch 600 Loss 1.8860 Accuracy 0.1429\n",
            "Epoch 9 Batch 650 Loss 1.8834 Accuracy 0.1430\n",
            "Epoch 9 Batch 700 Loss 1.8803 Accuracy 0.1430\n",
            "Epoch 9 Batch 750 Loss 1.8759 Accuracy 0.1431\n",
            "Epoch 9 Batch 800 Loss 1.8729 Accuracy 0.1432\n",
            "Epoch 9 Loss 1.8729 Accuracy 0.1432\n",
            "Time taken for 1 epoch: 238.55426740646362 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.7717 Accuracy 0.1458\n",
            "Epoch 10 Batch 50 Loss 1.8584 Accuracy 0.1442\n",
            "Epoch 10 Batch 100 Loss 1.8461 Accuracy 0.1444\n",
            "Epoch 10 Batch 150 Loss 1.8402 Accuracy 0.1442\n",
            "Epoch 10 Batch 200 Loss 1.8356 Accuracy 0.1443\n",
            "Epoch 10 Batch 250 Loss 1.8349 Accuracy 0.1445\n",
            "Epoch 10 Batch 300 Loss 1.8309 Accuracy 0.1446\n",
            "Epoch 10 Batch 350 Loss 1.8270 Accuracy 0.1446\n",
            "Epoch 10 Batch 400 Loss 1.8255 Accuracy 0.1446\n",
            "Epoch 10 Batch 450 Loss 1.8216 Accuracy 0.1448\n",
            "Epoch 10 Batch 500 Loss 1.8186 Accuracy 0.1448\n",
            "Epoch 10 Batch 550 Loss 1.8161 Accuracy 0.1449\n",
            "Epoch 10 Batch 600 Loss 1.8143 Accuracy 0.1449\n",
            "Epoch 10 Batch 650 Loss 1.8117 Accuracy 0.1450\n",
            "Epoch 10 Batch 700 Loss 1.8082 Accuracy 0.1450\n",
            "Epoch 10 Batch 750 Loss 1.8046 Accuracy 0.1452\n",
            "Epoch 10 Batch 800 Loss 1.8016 Accuracy 0.1452\n",
            "Saving checkpoint for epoch 10 at /content/gdrive/My Drive/transformer_caption/train/ckpt-2\n",
            "Epoch 10 Loss 1.8016 Accuracy 0.1452\n",
            "Time taken for 1 epoch: 240.64413332939148 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.7093 Accuracy 0.1448\n",
            "Epoch 11 Batch 50 Loss 1.8005 Accuracy 0.1458\n",
            "Epoch 11 Batch 100 Loss 1.7843 Accuracy 0.1459\n",
            "Epoch 11 Batch 150 Loss 1.7739 Accuracy 0.1460\n",
            "Epoch 11 Batch 200 Loss 1.7726 Accuracy 0.1463\n",
            "Epoch 11 Batch 250 Loss 1.7665 Accuracy 0.1465\n",
            "Epoch 11 Batch 300 Loss 1.7610 Accuracy 0.1466\n",
            "Epoch 11 Batch 350 Loss 1.7606 Accuracy 0.1465\n",
            "Epoch 11 Batch 400 Loss 1.7558 Accuracy 0.1467\n",
            "Epoch 11 Batch 450 Loss 1.7533 Accuracy 0.1467\n",
            "Epoch 11 Batch 500 Loss 1.7511 Accuracy 0.1467\n",
            "Epoch 11 Batch 550 Loss 1.7468 Accuracy 0.1469\n",
            "Epoch 11 Batch 600 Loss 1.7463 Accuracy 0.1469\n",
            "Epoch 11 Batch 650 Loss 1.7446 Accuracy 0.1470\n",
            "Epoch 11 Batch 700 Loss 1.7412 Accuracy 0.1471\n",
            "Epoch 11 Batch 750 Loss 1.7380 Accuracy 0.1472\n",
            "Epoch 11 Batch 800 Loss 1.7349 Accuracy 0.1473\n",
            "Epoch 11 Loss 1.7349 Accuracy 0.1473\n",
            "Time taken for 1 epoch: 241.53432250022888 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.6716 Accuracy 0.1549\n",
            "Epoch 12 Batch 50 Loss 1.7392 Accuracy 0.1480\n",
            "Epoch 12 Batch 100 Loss 1.7206 Accuracy 0.1478\n",
            "Epoch 12 Batch 150 Loss 1.7137 Accuracy 0.1477\n",
            "Epoch 12 Batch 200 Loss 1.7104 Accuracy 0.1478\n",
            "Epoch 12 Batch 250 Loss 1.7082 Accuracy 0.1481\n",
            "Epoch 12 Batch 300 Loss 1.7025 Accuracy 0.1483\n",
            "Epoch 12 Batch 350 Loss 1.7002 Accuracy 0.1483\n",
            "Epoch 12 Batch 400 Loss 1.6973 Accuracy 0.1483\n",
            "Epoch 12 Batch 450 Loss 1.6956 Accuracy 0.1484\n",
            "Epoch 12 Batch 500 Loss 1.6927 Accuracy 0.1485\n",
            "Epoch 12 Batch 550 Loss 1.6899 Accuracy 0.1486\n",
            "Epoch 12 Batch 600 Loss 1.6881 Accuracy 0.1487\n",
            "Epoch 12 Batch 650 Loss 1.6872 Accuracy 0.1487\n",
            "Epoch 12 Batch 700 Loss 1.6835 Accuracy 0.1488\n",
            "Epoch 12 Batch 750 Loss 1.6805 Accuracy 0.1489\n",
            "Epoch 12 Batch 800 Loss 1.6786 Accuracy 0.1490\n",
            "Epoch 12 Loss 1.6786 Accuracy 0.1490\n",
            "Time taken for 1 epoch: 240.16972613334656 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}